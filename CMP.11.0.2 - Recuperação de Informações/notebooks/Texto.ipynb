{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similaridade de texto (palavras)\n",
    "\n",
    "A similaridade de texto é utilizada para comparar palavras similares (que foram escritas de forma diferente).\n",
    "Alguns exemplos:\n",
    "\n",
    "* `São Paulo`\n",
    "* `Sao Paulo`\n",
    "*  `São Paulo   ` (tem espaço no final)\n",
    "* `são paulo`\n",
    "\n",
    "Todas as palavras acima se referem a cidade de São Paulo, mas se efetuar umas simples comparação de _strings_ (`str1 == str2`) o resultado será `False`.\n",
    "\n",
    "É possível tratar esses casos, como (`str.strip()`) para remover espaços no começo/final do texto.\n",
    "Outro detalhe é `str.lower()` ou `str.upper()` para manter tudo minúsculo/maiúsculo.\n",
    "Finalmente, é possível remover toda a acentuação para evitar problemas.\n",
    "\n",
    "Entretanto, essa não é uma boa adordagem (principalmente remover acentuação, as outras duas podem ser boas ideias) visto que estamos alterando o texto para encontrar o nome `São Paulo`.\n",
    "\n",
    "Uma alternativa é computar um número (entre 0 e 100, por exemplo) que indica o quão similares são duas palavras.\n",
    "Sendo que, 0 indica palavras completamente diferentes e 100 indica que as palavras são idênticas.\n",
    "Assim, podemos definir um limiar de forma que, qualquer similaridade acima de 80 aceitamos como algo válido.\n",
    "O valor de limiar vai variar com o contexto, por isso é importante testar várias abordagens.\n",
    "\n",
    "Existem várias métricas para calcular essa similaridade: https://en.wikipedia.org/wiki/Edit_distance\n",
    "\n",
    "Vamos trabalhar com a distância de Levenshtein: https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "\n",
    "A distância de Levenshtein calcula o número minímino de alterações (inserir, remover, substituir) que devemos fazer em uma string para que ela fique igual a outra.\n",
    "\n",
    "Tutorial `fuzzywuzzy` https://www.datacamp.com/community/tutorials/fuzzy-string-python\n",
    "\n",
    "Outra referência é a seção 3.3.3 na página 58 do livro [_An Introduction to Information Retrieval_](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distância de levenshtein\n",
    "import numpy as np\n",
    "\n",
    "def levenshtein(a, b):\n",
    "    pass\n",
    "\n",
    "assert levenshtein('oi', 'oi') == 0\n",
    "assert levenshtein('car', 'cat') == 1\n",
    "assert levenshtein('oi', 'ola') == 2\n",
    "assert levenshtein('cats', 'fast') == 3\n",
    "assert levenshtein('sitting', 'kitten') == 3\n",
    "assert levenshtein('sunday', 'saturday') == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/Projects/pos-furb-recuperacao/.venv/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('São Paulo', 'Sao Paulo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('São Paulo', 'sao paulo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('São Paulo', 'São Paulo    ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('Sao Paulo', 'São Paulo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, podemos ter uma lista de cidades e ver quais pessoas moram em `São Paulo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 90, 89, 67, 82, 82, 24, 24, 35]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pessoas = [\n",
    "    {'nome': 'Pessoa A', 'cidade': 'São Paulo'},\n",
    "    {'nome': 'Pessoa B', 'cidade': ' São Paulo '},\n",
    "    {'nome': 'Pessoa C', 'cidade': 'Sao Paulo'},\n",
    "    {'nome': 'Pessoa D', 'cidade': 'sao paulo'},\n",
    "    {'nome': 'Pessoa E', 'cidade': 'SaoPaulo'},\n",
    "    {'nome': 'Pessoa F', 'cidade': 'S. Paulo'},\n",
    "    {'nome': 'Pessoa G', 'cidade': 'Blumenau'},\n",
    "    {'nome': 'Pessoa H', 'cidade': 'Pomerode'},\n",
    "    {'nome': 'Pessoa I', 'cidade': 'Rio de Janeiro'},\n",
    "]\n",
    "\n",
    "scores = [fuzz.ratio('São Paulo', p['cidade']) for p in pessoas]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entretanto, nem sempre é possível comparar o texto diretamente.\n",
    "Por exemplo, o nome da cidade pode estar no meio de uma frase.\n",
    "Para isso, é possível fazer uma comparação parcial.\n",
    "\n",
    "`fuzz.ratio()` compara dois textos por completo, com a similaridade entre eles.\n",
    "\n",
    "`fuzz.partial_ratio()` faz a comparação em substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('São Paulo', 'João mudou-se para São Paulo em 2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('São Paulo', 'João mudou-se para São Paulo em 2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('São Paulo', 'João mudou-se para Sao Paulo em 2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro caso comum, é a ordem dos textos.\n",
    "\n",
    "Por exemplo, `Comparamos o Dispositivo A com o Dispositivo B` e `Comparamos o Dispositivo B com o Dispositivo A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('Dispositivo A com Dispositivo B', 'Comparamos Dispositivo A com Dispositivo B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('Dispositivo A com Dispositivo B', 'Comparamos dispositivo B com dispositivo A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_sort_ratio('Dispositivo A com Dispositivo B', 'Comparamos dispositivo B com dispositivo A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro caso é o uso da comparação `Dispositivo A com Dispositivo B` ou `Dispositivo A x Dispositivo B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_sort_ratio('Dispositivo A x Dispositivo B', 'Comparamos dispositivo B com dispositivo A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_set_ratio('Dispositivo A x Dispositivo B', 'Comparamos dispositivo B com dispositivo A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento de linguagem natural\n",
    "\n",
    "São técnicas utilizadas para trabalhar com texto que englobam desde _tokenização_ (separar uma string em palavras) até a tradução de texto.\n",
    "\n",
    "A similaridade de palavras acima demonstra um uso de processamento de linguagem natural.\n",
    "\n",
    "Nesta parte, vamos ver alguns conceitos básicos utilizando as bibliotecas `NLTK` e `spaCy`.\n",
    "\n",
    "Uma comparação entre `NLTK` e `spaCy` https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonte http://www.furb.br/web/1704/noticias/furb-esta-entre-4-melhores-de-sc-em-ranking-da-america-latina/7944\n",
    "texto = '''O Brasil desponta com as melhores Universidades da América Latina, com o total de 3 no top 10,\n",
    "seguida do Chile, Colômbia e México com 2 em cada país, além da Argentina com uma.\n",
    "No primeiro lugar está a Pontifícia Universidade Católica do Chile, seguida da Universidade de São Paulo (USP).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização\n",
    "\n",
    "Separara um texto em uma lista de palavras.\n",
    "Note que a forma mais simples de resolver esse problema é `texto.split(' ')` apenas separando por espaços.\n",
    "Entretanto, essa abordagem falha em um texto como `... fim do dia.`, onde `dia.` seria um token, mas deveria ser separdo em dois: `dia` e `.`.\n",
    "O mesmo acontece para nomes, `João B. da Silva`, é importante notar que `B.` não é o final da frase, mas uma abreviação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pode ser necessário executar\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "Brasil\n",
      "desponta\n",
      "com\n",
      "as\n",
      "melhores\n",
      "Universidades\n",
      "da\n",
      "América\n",
      "Latina\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(texto)\n",
    "for (i, token) in enumerate(tokens):\n",
    "    print(token)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-76a01d9c502b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-acecbfa52f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pode ser necessário instalar o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# python -m spacy download pt_core_news_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pt_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# pode ser necessário instalar o modelo\n",
    "# python -m spacy download pt_core_news_sm\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ac6884a77bfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-19b9bf8dde99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "for (i, token) in enumerate(doc):\n",
    "    print(token.text)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c695bfc8f19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# apenas os verbos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mverbos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'VERB'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mverbos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "# apenas os verbos\n",
    "verbos = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "verbos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de _stopwords_\n",
    "\n",
    "_Stopwords_ são palavras utilizadas para conectar texto (`de`, `a`, `e`, outros) e não estão ligadas diretamente ao sentido da frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pode ser necessário executar\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sws = stopwords.words('portuguese')\n",
    "sws[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'Brasil',\n",
       " 'desponta',\n",
       " 'melhores',\n",
       " 'Universidades',\n",
       " 'América',\n",
       " 'Latina',\n",
       " ',',\n",
       " 'total',\n",
       " '3']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_sem_stopwords = [t for t in tokens if t not in sws]\n",
    "print(len(tokens), len(tokens_sem_stopwords))\n",
    "tokens_sem_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[O, Brasil, desponta, melhores, Universidades, América, Latina, ,, o, total]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens_sem_stopwords = [t for t in doc if not t.is_stop]\n",
    "spacy_tokens_sem_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem\n",
    "\n",
    "_Stem_ é um método para reduzir uma palavra ao radical/raiz.\n",
    "Como os verbos são conjugados, pode ser difícil trabalhar com todas as variações.\n",
    "Uma forma de reduzir esse problema, é trabalhar apenas com o radical do verbo.\n",
    "Por: `copiar` -> `copi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pode ser necessário executar\n",
    "# import nltk\n",
    "# nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'necess'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"necessário\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'necess'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"necessidade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desnecess'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"desnecessário\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'brasil',\n",
       " 'despont',\n",
       " 'melhor',\n",
       " 'univers',\n",
       " 'amér',\n",
       " 'latin',\n",
       " ',',\n",
       " 'total',\n",
       " '3']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_stem = [stemmer.stem(t) for t in tokens_sem_stopwords]\n",
    "tokens_stem[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Enquanto _stemming_ reduz as palavras removendo partes no começo e final, _lemmatization_ reduz a palavra na forma do dicionário.\n",
    "\n",
    "Para mais informações: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'Brasil',\n",
       " 'despontar',\n",
       " 'melhorar',\n",
       " 'Universidades',\n",
       " 'América',\n",
       " 'Latina',\n",
       " ',',\n",
       " 'o',\n",
       " 'total']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = [t.lemma_ for t in spacy_tokens_sem_stopwords]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "_Term Frequency - Inverse Document Frequency_ é uma forma de calcular a importância de uma palavra em um documento.\n",
    "\n",
    "### Term Frequency\n",
    "\n",
    "Quantas vezes um termo (palavra) aparece em um documento.\n",
    "\n",
    "* $\\text{TF}_{t, d} = f_{t, d}$\n",
    "\n",
    "Sendo $t$ um termo e $d$ um documento, ou seja, $f_{t, d}$ é simplesmente a contagem do termo $t$ no documento $d$.\n",
    "\n",
    "O problema com a definição anterior é que determinados termos podem aparecer mais vezes simplesmente porque o documento é maior.\n",
    "Portanto, é necessário normalizar a contagem de termos.\n",
    "\n",
    "* $\\text{TF}_{t, d} = \\frac{f_{t, d}}{\\lvert d \\rvert}$\n",
    "\n",
    "Sendo $f_{t, d}$ a mesma definição acima e $\\lvert d \\rvert$ a quantidade total de termos no documento $d$.\n",
    "Essa é uma métrica normalizada, assim $\\sum{TF_{t, d}} = 1$.\n",
    "\n",
    "* $\\text{TF}_{t, d} = \\frac{f_{t, d}}{\\text{max}_{t'}f_{t', d}}$\n",
    "\n",
    "Essa definição é similar a anterior, mas ao invés de normalizar pela quantidade total de termos em $d$, a normalização é pela contagem do termo mais frequente $\\text{max}_{t'}f_{t', d}$.\n",
    "\n",
    "A vantagem de normalizar pelo termo mais frequente aparece quando os documentos são muito grandes (contém muitos termos). Veremos um exemplo a seguir.\n",
    "\n",
    "### Inverse Document Frequency\n",
    "\n",
    "Apenas as contagens de $\\text{TF}$ não representam o conteúdo de um documento, visto que existem palavras que aparecem mais frequentemente.\n",
    "\n",
    "Portanto, é necessário ponderar a importância de um determinado termo.\n",
    "\n",
    "Para isso, usa-se\n",
    "\n",
    "$\\text{IDF}_t = \\text{log}_2(\\frac{N}{n_t})$\n",
    "\n",
    "Sendo, $N$ a quantidade total de documentos e $n_t$ a quantidade de documentos que o termo $t$ aparece\n",
    "\n",
    "\n",
    "Assim que tiver calculado os valores de $\\text{TF}$ e $\\text{IDF}$ é possível calcular o valor final com\n",
    "\n",
    "$\\text{TF-IDF}_{t, d} = \\text{TF}_{t, d} \\times \\text{IDF}_t$\n",
    "\n",
    "\n",
    "Note que remover palavras muito comuns ( _stop words_ ) pode ser importante antes de calcular o \n",
    "$\\text{TF-IDF}_{t, d}$ visto que algumas palavras sem importância já foram removidas.\n",
    "\n",
    "Referências:\n",
    "* http://www.tfidf.com/\n",
    "* [Mining Massive Datasets, p. 7-9](http://www.mmds.org/)\n",
    "* [Introduction to Information Retrieval, p. 117-120](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)\n",
    "* [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "* [Clustering Text Documents using k-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)\n",
    "* [TdIdfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alguns documentos para tests\n",
    "documentos = []\n",
    "\n",
    "# Fonte: http://www.furb.br/web/1704/noticias/estudantes-participam-da-semana-global-de-empreendedorismo/7989\n",
    "documentos.append('''Estudantes de Tecnologia em Marketing da FURB, na disciplina Comunicação Empresarial, são os responsáveis pela comunicação local da Semana Global de Empreendedorismo. O evento, criado em 2007, tem como objetivo fortalecer e disseminar a cultura empreendedora e este ano ocorre de 18 a 24 de novembro, em mais de 170 países. A expectativa global é envolver mais de 10 milhões de pessoas em 35 mil eventos cadastrados.\n",
    " \n",
    "Em Blumenau a programação acontece nesta segunda, 18 e terça, 19 de novembro, no auditório da Biblioteca Universitária, campus 1 da FURB. “O evento é público, mas estamos direcionando aos estudantes da Universidade, começando por envolvê-los na divulgação local”, explica a professora de Comunicação Empresarial, Heloísa Rosa. Além dela, estão na organização local da Semana Global de Empreendedorismo os professores Camila Schmitt, Silvio Luís de Vasconcellos e Valter Augusto Krauss, todos ligados ao Centro de Ciências Sociais Aplicadas (CCSA) da FURB.\n",
    " \n",
    "As atividades iniciam nesta segunda-feira, 18, às 18h30, com palestra de Ionita Lunelli, analista do Sebrae/SC há mais de 20 anos. Ela fala sobre o Sebrae e o estímulo que promove ao empreendedorismo. Na sequência, o consultor para o desenvolvimento de startups do Instituto Nexxera | NexxLabs e criador da Metodologia TCC Start Up, João Geraldo Campos, apresenta Negócios com Propósito: Cases de Empreendedorismo Social. O convidado tem formação em Educação Empreendedora pela Babson College, Massachusetts e foi o vencedor do Prêmio Nacional de Educação Empreendedora ENDEAVOR Brasil 2016, com o projeto TCC Start Up, vinculado à Agência de Inovação e Empreendedorismo da Unisul (AGETEC).\n",
    " \n",
    "Na terça-feira, 19, também a partir das 18h30, a FURB recebe a equipe da Haco Etiquetas: Bruno Brandão (Gerente de Marketing), Silvio Triques (Coordenador de Marketing), Taís Prawucki (Planejamento e Criação) e Osiris Reis (Estratégias de Marketing Digital e Relacionamento) para a palestra Marketing 360º.\n",
    " \n",
    "Nos dois dias as palestras ocorrem no Auditório da Biblioteca Universitária, campus 1 da FURB, das 18h30 às 22h, a participação é gratuita e as inscrições podem ser feitas neste link.''')\n",
    "\n",
    "# Fonte: http://www.furb.br/web/1704/noticias/jornada-reune-especialistas-em-citopatologia-veterinaria/7988\n",
    "documentos.append('''Nos dias 22 e 23 de novembro, o auditório do Bloco J, campus 1 da FURB, será movimentado por uma série de apresentações e palestras que abordarão temas como o combate e detecção de doenças em animais como cães e gatos, por exemplo. A primeira edição da Jornada de Citopatologia Veterinária está sendo organizada pelo Grupo de Estudos em Patologia Veterinária da FURB, o GEPAV. A Jornada é voltada a acadêmicos, professores e profissionais com interesse na área da citologia, ou seja, o estudo do desenvolvimento e das funções das células nos animais.\n",
    " \n",
    "Conforme a professora do curso de Medicina Veterinária da FURB, Joelma Lucioli, que auxilia na organização do evento, “o objetivo é fornecer informações atualizadas e de qualidade, além de promover o intercâmbio técnico e científico entre o público envolvido neste conjunto de palestras e apresentações, contando com a participação de profissionais com ampla experiência em citopatologia”. Entre os especialistas que estarão presentes na jornada acadêmica estão o Dr. Breno Souza Salgado (UFES/ES), a Dra. Lidiane Narducci Monteiro (CODIVET/ES) e o mestre Leonardo Dourado da Costa (Instituto Qualittas).\n",
    " \n",
    "O evento tem início na sexta-feira, dia 22 de novembro, com a retirada de materiais e abertura oficial, que acontece a partir das 18h no auditório do Bloco J, campus 1. Às 19h, terá início palestra que discutirá a importância da citologia na conduta clínica, destacando erros e limitações. Em seguida, às 20h40, outro painel tratará da coleta e do processamento das amostras citológicas. Haverá um intervalo de 20 minutos entre uma palestra e outra.\n",
    " \n",
    "Já o sábado, 23, será um dia inteiro de apresentações e trocas de conhecimentos entre os profissionais envolvidos. A programação tem início às 8h da manhã, com a continuação do painel que aborda as amostras citológicas, trazendo as interpretações de resultados citológicos aferidos. Às 10h, outra apresentação irá explanar sobre o diagnóstico citológico de neoplasias de células redondas. Em seguida, às 11h30, haverá uma roda de conversas que irá discutir casos clínicos observados no cotidiano por profissionais da área.\n",
    " \n",
    "Durante a tarde, a partir das 14h, acontecerá palestra sobre o diagnóstico citológico das principais neoplasias epiteliais, que acometem a pele dos animais. Às 16h, outra palestra irá trazer os estudos sobre as principais doenças infecciosas de pele em cães e gatos. Logo após, haverá roda de discussão de casos clínicos observados por profissionais e os diagnósticos obtidos. Às 18 horas está marcada a cerimônia de encerramento.\n",
    " \n",
    "Para participar da Jornada de Citopatologia Veterinária, as inscrições deverão ser feitas através deste link até o dia 20 de novembro.\n",
    " \n",
    "O valor da inscrição para estudantes é de R$ 50,00 e para profissionais da área, R$ 100,00.\n",
    " \n",
    "Sócios da Associação Brasileira de Patologias Veterinárias (ABPV), terão 10% de desconto na inscrição. O pagamento pode ser feito através de depósito ou transferência bancária.\n",
    " \n",
    "Mais informações podem ser obtidas pelo e-mail gepavfurb@gmail.com.\n",
    " \n",
    "A 1ª Jornada Acadêmica de Patologia Veterinária é uma realização do Grupo de Estudos em Patologia Veterinária da FURB (GEPAV), com o apoio da Associação Brasileira de Patologia Veterinária (ABPV) e auxílio financeiro da Fundação de Amparo à Pesquisa do Estado de Santa Catarina (FAPESC), através da Chamada Pública 01/2019.''')\n",
    "\n",
    "# Fonte: http://www.furb.br/web/1704/noticias/pelo-terceiro-mes-seguido-variacao-de-precos-negativa/7984\n",
    "documentos.append('''O Índice de Variação Geral de Preços do município de Blumenau (IVGP) calculado pela FURB para o mês de outubro apontou uma redução de preços em 92, dos 511 itens pesquisados para compor o índice. Organizados em 25 subgrupos, no último mês, cinco subgrupos registraram alta, enquanto 12 permaneceram estáveis e oito variaram negativamente.\n",
    " \n",
    "Os produtos de limpeza registraram a maior baixa de preços: -3,55%, seguidos dos alimentos industrializados que variaram -1,68% e dos serviços públicos, -1,10%.\n",
    "Já os itens que registraram a maior alta de preços foram os materiais de construção, +2,38%, os alimentos semi-industrializados, +1,25% e as autopeças, + 1,13%.  \n",
    "Assim, a variação de preços no mês de outubro foi negativa, -0,29%, abaixo do intervalo esperado pelos economistas, que era de 0,00% e +0,50%. A variação acumulada nos últimos doze meses também está negativa, em -0,09%.\n",
    " \n",
    "É o terceiro mês seguido que o IVGP registra variação negativa: em agosto foi de -0,93%, em setembro, -0,67% e em outubro, -0,29%. No mesmo mês do ano passado, o IVGP registrava +0,59%, maior índice dos últimos 12 meses.\n",
    " \n",
    "Mantido o atual contexto, a variação geral de preços acumulada nos 12 meses, até dezembro de 2019, tende a configurar o cenário otimista, isto é, +3,66%, com uma variação média de 0,30% ao mês. \n",
    " \n",
    "Para acompanhar a série histórica do índice, calculado desde 1993, basta acessar www.furb.br/ivgp''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "docs_termos = []\n",
    "for doc in documentos:\n",
    "    termos = word_tokenize(doc)\n",
    "    termos = (t for t in termos if t not in sws)\n",
    "    termos = (t for t in termos if len(t) > 1)\n",
    "    termos = (t for t in termos if t not in string.punctuation)\n",
    "    termos = (t.lower() for t in termos)\n",
    "    termos = list(termos)\n",
    "    docs_termos.append(termos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# também é possível usar\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "dv = DictVectorizer()\n",
    "X = dv.fit_transform(Counter(d) for d in docs_termos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 1., 3., 0., 3., 2., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 2., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 2., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 2., 1., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 3., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 1., 3.,\n",
       "        5., 2., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 2., 0., 0., 0., 1., 1., 2., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 0., 0., 5., 0., 0., 0., 0., 1., 1.,\n",
       "        3., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 1., 3., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 5., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        2., 1., 0., 0., 0., 1., 0., 2., 1., 1., 1., 0., 1., 2., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 2., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        2., 0., 0., 1., 1., 0., 0., 0., 2., 1., 1., 1., 0., 2., 1., 0.,\n",
       "        0., 0., 0., 0., 1., 2., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 1., 1., 2., 2., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, :].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '170', '18', '18h30', '19', '20', '2007', '2016', '22h',\n",
       "       '24', '35', '360º', 'acontece', 'agetec', 'agência', 'além',\n",
       "       'analista', 'ano', 'anos', 'aplicadas', 'apresenta', 'as',\n",
       "       'atividades', 'auditório', 'augusto', 'babson', 'biblioteca',\n",
       "       'blumenau', 'brandão', 'brasil', 'bruno', 'cadastrados', 'camila',\n",
       "       'campos', 'campus', 'cases', 'ccsa', 'centro', 'ciências',\n",
       "       'college', 'começando', 'comunicação', 'consultor', 'convidado',\n",
       "       'coordenador', 'criado', 'criador', 'criação', 'cultura',\n",
       "       'desenvolvimento', 'dias', 'digital', 'direcionando', 'disciplina',\n",
       "       'disseminar', 'divulgação', 'dois', 'educação', 'ela', 'em',\n",
       "       'empreendedora', 'empreendedorismo', 'empresarial', 'endeavor',\n",
       "       'envolver', 'envolvê-los', 'equipe', 'estratégias', 'estudantes',\n",
       "       'estímulo', 'etiquetas', 'evento', 'eventos', 'expectativa',\n",
       "       'explica', 'fala', 'feitas', 'formação', 'fortalecer', 'furb',\n",
       "       'geraldo', 'gerente', 'global', 'gratuita', 'haco', 'heloísa',\n",
       "       'iniciam', 'inovação', 'inscrições', 'instituto', 'ionita', 'joão',\n",
       "       'krauss', 'ligados', 'link', 'local', 'lunelli', 'luís',\n",
       "       'marketing', 'massachusetts', 'metodologia', 'mil', 'milhões',\n",
       "       'na', 'nacional', 'negócios', 'nesta', 'neste', 'nexxera',\n",
       "       'nexxlabs', 'nos', 'novembro', 'objetivo', 'ocorre', 'ocorrem',\n",
       "       'organização', 'osiris', 'palestra', 'palestras', 'participação',\n",
       "       'partir', 'países', 'pessoas', 'planejamento', 'podem', 'prawucki',\n",
       "       'professora', 'professores', 'programação', 'projeto', 'promove',\n",
       "       'propósito', 'prêmio', 'público', 'recebe', 'reis',\n",
       "       'relacionamento', 'responsáveis', 'rosa', 'schmitt', 'sebrae',\n",
       "       'sebrae/sc', 'segunda', 'segunda-feira', 'semana', 'sequência',\n",
       "       'ser', 'silvio', 'sobre', 'sociais', 'social', 'start', 'startups',\n",
       "       'taís', 'tcc', 'tecnologia', 'terça', 'terça-feira', 'todos',\n",
       "       'triques', 'unisul', 'universidade', 'universitária', 'up',\n",
       "       'valter', 'vasconcellos', 'vencedor', 'vinculado'], dtype='<U21')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# termos no documento\n",
    "mask = X[0, :].toarray().flatten() > 0\n",
    "np.array(dv.get_feature_names())[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['+0,50', 'já', 'jornada', 'joelma', 'ivgp', 'itens', 'irá',\n",
       "       'início', 'intervalo', 'interpretações'], dtype='<U21')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top-k termos\n",
    "k = 10\n",
    "args = np.argsort(X[0, :].toarray().flatten())\n",
    "np.array(dv.get_feature_names())[args[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer(norm='l1')  # l1 norm, divide pela quantidade de termos no documento\n",
    "X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['+0,50', 'já', 'jornada', 'joelma', 'ivgp', 'itens', 'irá',\n",
       "       'início', 'intervalo', 'interpretações'], dtype='<U21')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top-k termos\n",
    "k = 10\n",
    "args = np.argsort(X[0, :].toarray().flatten())\n",
    "np.array(dv.get_feature_names())[args[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) tagging\n",
    "\n",
    "Anota as palavras de acordo com a sua função linguística na frase.\n",
    "Ou seja, indica se uma palavra é um verbo, substantivo, pontuação, outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = [t.pos_ for t in spacy_tokens_sem_stopwords]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desponta\n",
      "top\n",
      "seguida\n",
      "está\n",
      "seguida\n"
     ]
    }
   ],
   "source": [
    "# pegando apenas os verbos\n",
    "for token in filter(lambda x: x.pos_ == 'VERB', doc):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total\n",
      "país\n",
      "lugar\n",
      "Pontifícia\n"
     ]
    }
   ],
   "source": [
    "# pegando apenas os substantivos\n",
    "for token in filter(lambda x: x.pos_ == 'NOUN', doc):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasil\n",
      "Universidades\n",
      "América\n",
      "Latina\n",
      "10\n",
      "do\n",
      "Chile\n",
      "Colômbia\n",
      "México\n",
      "Argentina\n",
      "Universidade\n",
      "Católica\n",
      "Chile\n",
      "da\n",
      "Universidade\n",
      "São\n",
      "Paulo\n",
      "USP\n"
     ]
    }
   ],
   "source": [
    "# pegando apenas os nomes próprios\n",
    "for token in filter(lambda x: x.pos_ == 'PROPN', doc):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Reconhece e classifica entidades no texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasil LOC\n",
      "Universidades da América Latina PER\n",
      "Chile LOC\n",
      "Colômbia LOC\n",
      "México LOC\n",
      "Argentina LOC\n",
      "Pontifícia Universidade Católica do Chile LOC\n",
      "Universidade de São Paulo LOC\n",
      "USP LOC\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
